{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396e713a-4480-4b4e-856b-b0db55098c83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de sequências: 83\n",
      "       header                                           sequence\n",
      "0  A0A384TSM3  MTAILERRESESLWGRFCNWITSTENRLYIGWFGVLMIPTLLTATS...\n",
      "1  A0A384SYQ7  MKTLYSLRRFYPVETLFNGTLALAGRDQETTGFAWWAGNARLINLS...\n",
      "2  A0A384SYR4  MSPQTETKASVGFKAGVKDYKLNYYTPDYETKDTDILAAFRVTPQP...\n",
      "3  A0A384SYS1            MTIDRTYPIFTVRWLAVHGLAVPTVSFLGSISAMQFIQR\n",
      "4  A0A384SYS4  MQGRLSAWLVKHGLVHRSLGFDYQGIETLQIKPEDWHSIAVILYVY...\n",
      "Dispositivo: cpu\n"
     ]
    }
   ],
   "source": [
    "import esm\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "\n",
    "def extract_and_save_embeddings_to_csv(df_fasta, model, alphabet, device, batch_size=32, save_every=1000, output_prefix=\"esm2_embeddings\"):\n",
    "    #device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    #device = 'cpu'\n",
    "    print(\"Dispositivo:\", device)\n",
    "    \n",
    "    batch_converter = alphabet.get_batch_converter()\n",
    "\n",
    "    all_rows = []  # Cada item será: [header, f1, f2, ..., f640]\n",
    "    saved_batches = 0\n",
    "\n",
    "    batch_sequences = []\n",
    "    batch_headers = []\n",
    "\n",
    "    for i, row in enumerate(df_fasta.itertuples(index=False)):\n",
    "        header, sequence = row.header, row.sequence\n",
    "        batch_headers.append(header)\n",
    "        batch_sequences.append((header, sequence))\n",
    "\n",
    "        if len(batch_sequences) == batch_size or i == len(df_fasta) - 1:\n",
    "            try:\n",
    "                batch_labels, batch_strs, batch_tokens = batch_converter(batch_sequences)\n",
    "                batch_tokens = batch_tokens.to(device)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    results = model(batch_tokens, repr_layers=[30], return_contacts=False)\n",
    "                    token_representations = results[\"representations\"][30]\n",
    "\n",
    "                    for j, tokens_len in enumerate((batch_tokens != alphabet.padding_idx).sum(1)):\n",
    "                        seq_embedding = token_representations[j, 1:tokens_len - 1].mean(0).cpu().numpy()\n",
    "                        all_rows.append([batch_headers[j]] + seq_embedding.tolist())\n",
    "\n",
    "            except RuntimeError as e:\n",
    "                print(f\"Erro de memória em {batch_headers}: {e}\")\n",
    "                torch.cuda.empty_cache()\n",
    "                continue\n",
    "\n",
    "            # Libera GPU\n",
    "            del batch_tokens, results, token_representations\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "            batch_sequences = []\n",
    "            batch_headers = []\n",
    "\n",
    "            if (i + 1) % save_every == 0 or i == len(df_fasta) - 1:\n",
    "                save_path = f\"{output_prefix}\"\n",
    "                print(f\"Salvando {len(all_rows)} embeddings em {save_path}...\")\n",
    "\n",
    "                df_out = pd.DataFrame(all_rows)\n",
    "                df_out.to_csv(save_path, index=False, header=False)\n",
    "                saved_batches += 1\n",
    "                all_rows = []\n",
    "\n",
    "    print(\"Extração finalizada.\")\n",
    "\n",
    "def make_fasta(fasta_path):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model, alphabet = esm.pretrained.esm2_t30_150M_UR50D()\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    batch_converter = alphabet.get_batch_converter()\n",
    "    headers = []\n",
    "    seqs = []\n",
    "    \n",
    "    with open(fasta_path, 'r') as f:\n",
    "        current_header = None\n",
    "        current_seq = []\n",
    "    \n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line.startswith(\">\"):\n",
    "                if current_header is not None:\n",
    "                    headers.append(current_header)\n",
    "                    seqs.append(''.join(current_seq))\n",
    "                current_header = line[1:].split()[0]\n",
    "                current_seq = []\n",
    "            else:\n",
    "                current_seq.append(line)\n",
    "        # última seq\n",
    "        if current_header is not None:\n",
    "            headers.append(current_header)\n",
    "            seqs.append(''.join(current_seq))\n",
    "    \n",
    "    df_fasta = pd.DataFrame({\"header\": headers, \"sequence\": seqs})\n",
    "    print(f\"Total de sequências: {len(df_fasta)}\")\n",
    "    print(df_fasta.head())\n",
    "    return df_fasta, device, model, alphabet\n",
    "\n",
    "fasta_path = \"jabuticaba/jabuticaba.fasta\"\n",
    "output = \"jabuticaba/jabuticaba_feats.csv\"\n",
    "\n",
    "df_fasta, device, model, alphabet = make_fasta(fasta_path)\n",
    "extract_and_save_embeddings_to_csv(df_fasta, model, alphabet, device, batch_size=1, save_every=10000000, output_prefix=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01dac5c5-824d-48a7-9b06-e0e38cb1e584",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff51b34-6a77-4e43-8530-88d15f2511af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a75818d-9469-49bd-9ba6-e049d3af76f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "feats_path = \"jabuticaba/jabuticaba_feats.csv\"\n",
    "ints_path = \"jabuticaba/jabuticaba_interactions.csv\"\n",
    "\n",
    "\n",
    "df_feats = pd.read_csv(feats_path, header=None)\n",
    "df_feats = df_feats.rename(columns={0: \"protein1\"})\n",
    "df_combined = pd.read_csv(ints_path)\n",
    "df_combined = df_combined[['protein1','protein2']]\n",
    "df_merged = df_combined.merge(df_feats, on=\"protein1\", how=\"left\")\n",
    "df_feats = df_feats.rename(columns={'protein1': \"protein2\"})\n",
    "df_merged = df_merged.merge(df_feats, on=\"protein2\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "748b0098-a8d5-44c3-b877-196fca4666b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = df_merged.columns[2:]\n",
    "X = df_merged[feature_cols]\n",
    "del df_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3e9df1f3-fe05-49b5-904c-3e1d850b8ad0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bruno\\anaconda3\\envs\\environment\\lib\\site-packages\\sklearn\\base.py:348: InconsistentVersionWarning: Trying to unpickle estimator StandardScaler from version 1.6.1 when using version 1.3.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "C:\\Users\\Bruno\\anaconda3\\envs\\environment\\lib\\site-packages\\sklearn\\base.py:348: InconsistentVersionWarning: Trying to unpickle estimator LabelBinarizer from version 1.6.1 when using version 1.3.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "__randomstate_ctor() takes from 0 to 1 positional arguments but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m mlp_loaded \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodelos/MLP_model_planta(512, 256, 128, 64)_e25_920sc.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      9\u001b[0m scaler \u001b[38;5;241m=\u001b[39m joblib\u001b[38;5;241m.\u001b[39mload(scaler_path)\n\u001b[1;32m---> 10\u001b[0m mlp_loaded \u001b[38;5;241m=\u001b[39m \u001b[43mjoblib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmlp_loaded\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m#MLP_model_planta(256, 128, 64, 16)_e10_900sc.pkl\")\u001b[39;00m\n\u001b[0;32m     11\u001b[0m X_scaled \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39mtransform(X)\n\u001b[0;32m     14\u001b[0m y_proba \u001b[38;5;241m=\u001b[39m mlp_loaded\u001b[38;5;241m.\u001b[39mpredict_proba(X_scaled)[:, \u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\environment\\lib\\site-packages\\joblib\\numpy_pickle.py:749\u001b[0m, in \u001b[0;36mload\u001b[1;34m(filename, mmap_mode, ensure_native_byte_order)\u001b[0m\n\u001b[0;32m    744\u001b[0m                 \u001b[38;5;28;01mreturn\u001b[39;00m load_compatibility(fobj)\n\u001b[0;32m    746\u001b[0m             \u001b[38;5;66;03m# A memory-mapped array has to be mapped with the endianness\u001b[39;00m\n\u001b[0;32m    747\u001b[0m             \u001b[38;5;66;03m# it has been written with. Other arrays are coerced to the\u001b[39;00m\n\u001b[0;32m    748\u001b[0m             \u001b[38;5;66;03m# native endianness of the host system.\u001b[39;00m\n\u001b[1;32m--> 749\u001b[0m             obj \u001b[38;5;241m=\u001b[39m \u001b[43m_unpickle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    750\u001b[0m \u001b[43m                \u001b[49m\u001b[43mfobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    751\u001b[0m \u001b[43m                \u001b[49m\u001b[43mensure_native_byte_order\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_native_byte_order\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    752\u001b[0m \u001b[43m                \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    753\u001b[0m \u001b[43m                \u001b[49m\u001b[43mmmap_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidated_mmap_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    754\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    756\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\environment\\lib\\site-packages\\joblib\\numpy_pickle.py:626\u001b[0m, in \u001b[0;36m_unpickle\u001b[1;34m(fobj, ensure_native_byte_order, filename, mmap_mode)\u001b[0m\n\u001b[0;32m    624\u001b[0m obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    625\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 626\u001b[0m     obj \u001b[38;5;241m=\u001b[39m \u001b[43munpickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    627\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m unpickler\u001b[38;5;241m.\u001b[39mcompat_mode:\n\u001b[0;32m    628\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    629\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe file \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m has been generated with a \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    630\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjoblib version less than 0.10. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    633\u001b[0m             stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,\n\u001b[0;32m    634\u001b[0m         )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\environment\\lib\\pickle.py:1213\u001b[0m, in \u001b[0;36m_Unpickler.load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1211\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEOFError\u001b[39;00m\n\u001b[0;32m   1212\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, bytes_types)\n\u001b[1;32m-> 1213\u001b[0m         \u001b[43mdispatch\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _Stop \u001b[38;5;28;01mas\u001b[39;00m stopinst:\n\u001b[0;32m   1215\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m stopinst\u001b[38;5;241m.\u001b[39mvalue\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\environment\\lib\\pickle.py:1590\u001b[0m, in \u001b[0;36m_Unpickler.load_reduce\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1588\u001b[0m args \u001b[38;5;241m=\u001b[39m stack\u001b[38;5;241m.\u001b[39mpop()\n\u001b[0;32m   1589\u001b[0m func \u001b[38;5;241m=\u001b[39m stack[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m-> 1590\u001b[0m stack[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: __randomstate_ctor() takes from 0 to 1 positional arguments but 2 were given"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, precision_score, f1_score\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "\n",
    "scaler_path = \"modelos/scaler_planta_900sc.pkl\"\n",
    "mlp_loaded = \"modelos/MLP_model_planta(512, 256, 128, 64)_e25_920sc.pkl\"\n",
    "\n",
    "scaler = joblib.load(scaler_path)\n",
    "mlp_loaded = joblib.load(mlp_loaded)  #MLP_model_planta(256, 128, 64, 16)_e10_900sc.pkl\")\n",
    "X_scaled = scaler.transform(X)\n",
    "\n",
    "\n",
    "y_proba = mlp_loaded.predict_proba(X_scaled)[:, 1]\n",
    "'''y_pred = mlp_loaded.predict(X_scaled)\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(y, y_pred).ravel()\n",
    "especificidade = tn / (tn + fp)\n",
    "sensibilidade = tp / (tp + fn)\n",
    "acuracia = (tp + tn) / (tp + tn + fp + fn)\n",
    "precisao = tp / (tp + fp)\n",
    "f1 = 2 * (precisao * sensibilidade) / (precisao + sensibilidade)\n",
    "\n",
    "print(\"Acurácia:\", acuracia)\n",
    "print(\"Precisão:\", precisao)\n",
    "print(\"Sensibilidade:\", sensibilidade)\n",
    "print(\"Especificidade:\", especificidade)\n",
    "print(\"F1:\", f1)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df87b73f-a1f5-4700-9bb4-0c2f32f6fcfe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a11c31-c4be-462b-84af-d88409f79705",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4614c97-bf88-4054-8e90-04092784ca0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "makeblastdb_path = r\"C:\\Users\\Bruno\\Downloadsncbi-blast-2.16.0+\\bin\\makeblastdb.exe\"\n",
    "input_fasta = r\"Metazoa\\seg_ali\\fasta\\plantas_0_50_920sc_max1000.fasta\"\n",
    "cmd = [\n",
    "    makeblastdb_path,\n",
    "    \"-in\", input_fasta,\n",
    "    \"-dbtype\", \"prot\"  # Para banco de proteínas\n",
    "]\n",
    "try:\n",
    "    result = subprocess.run(cmd, capture_output=True, text=True, check=True)\n",
    "    print(\"Banco de dados criado com sucesso!\")\n",
    "    print(result.stdout)\n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(\"Erro ao criar o banco de dados:\")\n",
    "    print(e.stderr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3c3b2f-681a-4a18-8815-c7780cc371cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "blastp_path = r\"C:\\Users\\Bruno\\Downloads\\ncbi-blast-2.16.0+\\bin\\blastp.exe\"\n",
    "query_fasta = \"Metazoa/jabuticaba/jabuticaba.fasta\"\n",
    "output_file = \"Metazoa/jabuticaba/alinhamento_planta920_0-50sp_e_jabuticaba.txt\"     \n",
    "db_path = r\"Metazoa\\seg_ali\\fasta\\db_920\\plantas_0_50_920sc_max1000.fasta\"  #r\"modelo_200sp\\0-200sp_db\\string_200sp_max1000.fasta\"\n",
    "\n",
    "cmd = [\n",
    "    blastp_path,\n",
    "    \"-query\", query_fasta,\n",
    "    \"-db\", db_path,\n",
    "    \"-out\", output_file,\n",
    "    \"-outfmt\",  \"6 qseqid sseqid pident ppos nident mismatch gapopen qlen slen length qstart qend sstart send evalue bitscore\",\n",
    "    \"-max_target_seqs\", \"5\",\n",
    "    \"-num_threads\", \"16\" \n",
    "]\n",
    "\n",
    "\n",
    "# Executa\n",
    "try:\n",
    "    result = subprocess.run(cmd, capture_output=True, text=True, check=True)\n",
    "    print(\"BLAST concluído com sucesso!\")\n",
    "    print(f\"Resultados salvos em: {output_file}\")\n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(\"Erro ao executar o BLAST:\")\n",
    "    print(e.stderr)\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "colunas = [\n",
    "    \"qseqid\", \"sseqid\", \"pident\", \"ppos\", \"nident\", \"mismatch\", \"gapopen\",\n",
    "    \"qlen\", \"slen\", \"length\", \"qstart\", \"qend\", \"sstart\", \"send\", \"evalue\", \"bitscore\"\n",
    "]\n",
    "\n",
    "df = pd.read_csv(output_file, sep=\"\\t\", names=colunas)\n",
    "df.to_csv(output_file, sep=\"\\t\", index=False)\n",
    "print(\"Arquivo salvo com cabeçalho.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc621c5-4b4e-4e2f-a6ae-26a5828c262f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "colunas = [\n",
    "    \"qseqid\", \"sseqid\", \"pident\", \"ppos\", \"nident\", \"mismatch\", \"gapopen\",\n",
    "    \"qlen\", \"slen\", \"length\", \"qstart\", \"qend\", \"sstart\", \"send\", \"evalue\", \"bitscore\"\n",
    "]\n",
    "df = pd.read_csv(\n",
    "    output_file,\n",
    "    sep=\"\\t\",\n",
    "    header=None,\n",
    "    names=colunas\n",
    ")\n",
    "\n",
    "df_max = df.loc[df.groupby(\"qseqid\")[\"pident\"].idxmax()]\n",
    "\n",
    "df_top5 = df.sort_values(by=[\"qseqid\", \"pident\"], ascending=[True, False])\n",
    "df_top5 = df_top5.groupby(\"qseqid\").head(1)\n",
    "df_max = df_top5.groupby(\"qseqid\")[\"pident\"].mean().reset_index()\n",
    "\n",
    "print(len(df_max))\n",
    "\n",
    "plt.hist(df_max[\"pident\"], bins=50, edgecolor='black')\n",
    "plt.xlabel(\"Identidade (%)\")\n",
    "plt.ylabel(\"Frequência\")\n",
    "plt.title(\"Histograma do maior pident por qseqid\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "364f5800-7d63-4ae8-941e-8915bd351ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def filtrar_interacoes_por_dois_pidents(df_primeiro, df_combined, intervalo_max, intervalo_min):\n",
    "    \"\"\"\n",
    "    Filtra df_combined para manter apenas interações onde:\n",
    "    - As duas proteínas estão presentes em df_primeiro.\n",
    "    - O MAIOR pident entre as duas proteínas está dentro de intervalo_max.\n",
    "    - O MENOR pident entre as duas proteínas está dentro de intervalo_min.\n",
    "\n",
    "    Parâmetros:\n",
    "    - df_primeiro: DataFrame com colunas ['qseqid', 'pident']\n",
    "    - df_combined: DataFrame com colunas ['protein1', 'protein2', 'Label']\n",
    "    - intervalo_max: str no formato \"inicio-fim\", ex: \"40-60\"\n",
    "    - intervalo_min: str no formato \"inicio-fim\", ex: \"80-100\"\n",
    "\n",
    "    Retorna:\n",
    "    - DataFrame filtrado.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        inicio_max, fim_max = map(float, intervalo_max.split(\"-\"))\n",
    "        inicio_min, fim_min = map(float, intervalo_min.split(\"-\"))\n",
    "    except:\n",
    "        raise ValueError(\"Os intervalos devem estar no formato 'inicio-fim', por exemplo '40-60'.\")\n",
    "\n",
    "    # Conjunto de proteínas válidas\n",
    "    proteinas_validas = set(df_primeiro[\"qseqid\"])\n",
    "    \n",
    "    # Filtra só as interações onde as duas proteínas estão presentes\n",
    "    df_filtrado = df_combined[\n",
    "        (df_combined[\"protein1\"].isin(proteinas_validas)) &\n",
    "        (df_combined[\"protein2\"].isin(proteinas_validas))\n",
    "    ].copy()\n",
    "\n",
    "    # Dicionário com pident de cada proteína\n",
    "    pident_dict = df_primeiro.set_index(\"qseqid\")[\"pident\"].to_dict()\n",
    "\n",
    "    # Mapeia o pident\n",
    "    df_filtrado[\"pident1\"] = df_filtrado[\"protein1\"].map(pident_dict)\n",
    "    df_filtrado[\"pident2\"] = df_filtrado[\"protein2\"].map(pident_dict)\n",
    "\n",
    "    # Calcula máximo e mínimo\n",
    "    df_filtrado[\"pident_max\"] = df_filtrado[[\"pident1\", \"pident2\"]].max(axis=1)\n",
    "    df_filtrado[\"pident_min\"] = df_filtrado[[\"pident1\", \"pident2\"]].min(axis=1)\n",
    "\n",
    "    # Aplica os dois filtros\n",
    "    df_resultado = df_filtrado[\n",
    "        (df_filtrado[\"pident_max\"] > inicio_max) & (df_filtrado[\"pident_max\"] <= fim_max) &\n",
    "        (df_filtrado[\"pident_min\"] > inicio_min) & (df_filtrado[\"pident_min\"] <= fim_min)\n",
    "    ]\n",
    "\n",
    "    #print(f\"Total de interações com pident_max na faixa {intervalo_max}% e pident_min na faixa {intervalo_min}%: {len(df_resultado)}\")\n",
    "    return df_resultado\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix\n",
    "\n",
    "def avalia_df(df_resultado, y, y_pred, y_proba, faixa1, faixa2, plot_hist=True):\n",
    "    \"\"\"\n",
    "    df_resultado: DataFrame filtrado que define os índices a serem usados\n",
    "    y, y_pred, y_proba: arrays ou Series com rótulos verdadeiros, predições e probabilidades\n",
    "    faixa1, faixa2: strings indicando as faixas de origem (ex: \"0-40\", \"40-60\")\n",
    "    plot_hist: se True, plota o histograma final\n",
    "    \"\"\"\n",
    "\n",
    "    indices_filtrados = df_resultado.index\n",
    "\n",
    "    # Seleciona as amostras correspondentes\n",
    "    y_filtrado = y.loc[indices_filtrados] if hasattr(y, 'loc') else y[indices_filtrados]\n",
    "    y_pred_filtrado = y_pred.loc[indices_filtrados] if hasattr(y_pred, 'loc') else y_pred[indices_filtrados]\n",
    "    y_proba_filtrado = y_proba.loc[indices_filtrados] if hasattr(y_proba, 'loc') else y_proba[indices_filtrados]\n",
    "\n",
    "    # Transforma em DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        'y_true': y_filtrado,\n",
    "        'y_pred': y_pred_filtrado,\n",
    "        'y_proba': y_proba_filtrado\n",
    "    })\n",
    "\n",
    "    # Balanceamento\n",
    "    df_pos = df[df['y_true'] == 1]\n",
    "    df_neg = df[df['y_true'] == 0]\n",
    "    print(len(df_pos), len(df_neg))\n",
    "    min_count = min(len(df_pos), len(df_neg))\n",
    "    df_balanced = pd.concat([\n",
    "        df_pos.sample(n=min_count, random_state=42),\n",
    "        df_neg.sample(n=min_count, random_state=42)\n",
    "    ]).sample(frac=1, random_state=42)\n",
    "    print(len(df_balanced), faixa1, faixa2)\n",
    "\n",
    "    # Acurácia com predição clássica\n",
    "    acc = accuracy_score(df_balanced['y_true'], df_balanced['y_pred'])\n",
    "\n",
    "    resultados = []\n",
    "    thresholds = [0.5] #, 0.7, 0.9, 0.95, 0.99, 0.995, 0.999, 0.9999, 0.99999]\n",
    "\n",
    "    for thresh in thresholds:\n",
    "        y_pred_thresh = (df_balanced['y_proba'] >= thresh).astype(int)\n",
    "        prec = precision_score(df_balanced['y_true'], y_pred_thresh, zero_division=0)\n",
    "        rec = recall_score(df_balanced['y_true'], y_pred_thresh, zero_division=0)\n",
    "        tn, fp, fn, tp = confusion_matrix(df_balanced['y_true'], y_pred_thresh).ravel()\n",
    "        espec = tn / (tn + fp) if (tn + fp) > 0 else 0.0\n",
    "\n",
    "        resultados.append({\n",
    "            'Faixa1': faixa1,\n",
    "            'Faixa2': faixa2,\n",
    "            'Threshold': thresh,\n",
    "            'Precisão': prec,\n",
    "            'Recall': rec,\n",
    "            'Especificidade': espec,\n",
    "            'Acurácia_Padrao': acc\n",
    "        })\n",
    "\n",
    "    df_resultado_final = pd.DataFrame(resultados)\n",
    "\n",
    "    # Histograma opcional\n",
    "    if plot_hist:\n",
    "        probas_pos = df_balanced[df_balanced['y_true'] == 1]['y_proba']\n",
    "        probas_neg = df_balanced[df_balanced['y_true'] == 0]['y_proba']\n",
    "\n",
    "        plt.hist(probas_pos, bins=20, edgecolor='black', density=True, alpha=0.5, label='Positivas (y_true = 1)', color='blue')\n",
    "        plt.hist(probas_neg, bins=20, edgecolor='black', density=True, alpha=0.5, label='Negativas (y_true = 0)', color='red')\n",
    "\n",
    "        plt.xlabel(\"Probabilidade da Classe Positiva\")\n",
    "        plt.ylabel(\"Densidade\")\n",
    "        plt.title(f\"Histograma - {faixa1} x {faixa2} - {len(df_balanced)} Amostras Balanceadas\")\n",
    "        plt.legend()\n",
    "        #plt.show()\n",
    "\n",
    "    return df_resultado_final"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
